{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the CNN for age prediction\n",
    "class AgePredictorCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AgePredictorCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.dropout2 = nn.Dropout(0.25)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.dropout3 = nn.Dropout(0.25)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
    "        self.fc_dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.dropout3(x)\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc_dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ABHIRAM\\AppData\\Local\\Temp\\ipykernel_13264\\3867343072.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AgePredictorCNN(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout1): Dropout(p=0.25, inplace=False)\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (dropout2): Dropout(p=0.25, inplace=False)\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (dropout3): Dropout(p=0.25, inplace=False)\n",
       "  (fc1): Linear(in_features=4096, out_features=128, bias=True)\n",
       "  (fc_dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved model\n",
    "model_path = r'D:\\ml_projects\\haar_caascade\\age_predictor_cnn_new.pth'\n",
    "model = AgePredictorCNN()\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load cascades\n",
    "face_cascade = cv2.CascadeClassifier('D:\\ml_projects\\haar_caascade\\cascades\\haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('D:\\ml_projects\\haar_caascade\\cascades\\haarcascade_eye.xml')\n",
    "\n",
    "# Define transformation with actual input dimensions\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # Resize to standard dimensions\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "down code is almost perfect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best code sofar!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "ages = []\n",
    "frame_count = 0\n",
    "max_observations = 19  # Set the number of observations to consider\n",
    "drowsiness_count = 0   # Counter for drowsiness frames\n",
    "drowsiness_threshold = 5  # Number of frames to consider as drowsy\n",
    "\n",
    "# Open the video capture (0 for the default camera)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the frame\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.2, minNeighbors=6, minSize=(30, 30))\n",
    "\n",
    "    # List to hold age predictions for this frame\n",
    "    frame_ages = []\n",
    "\n",
    "    # Draw rectangles around the faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Extract the face ROI\n",
    "        face_roi = gray[y + 10:y + h - 10, x + 25:x + w - 25]\n",
    "\n",
    "        # Convert the NumPy array (face ROI) to a PIL image\n",
    "        face_pil = Image.fromarray(face_roi)  # Convert to PIL Image\n",
    "        \n",
    "        # Apply transformations to the PIL image\n",
    "        face_tensor = transform(face_pil).unsqueeze(0)  # Add batch dimension\n",
    "        \n",
    "        # Now detect eyes within the detected face region\n",
    "        eyes = eye_cascade.detectMultiScale(face_roi, scaleFactor=1.2, minNeighbors=7, minSize=(30, 30))\n",
    "        \n",
    "        # Check if no eyes are detected (indicating potential drowsiness)\n",
    "        if len(eyes) == 0:\n",
    "            drowsiness_count += 1  # Increment drowsiness counter if no eyes are detected\n",
    "        else:\n",
    "            drowsiness_count = 0  # Reset the counter if at least one eye is detected\n",
    "        \n",
    "        # Determine if the person is drowsy\n",
    "        if drowsiness_count > drowsiness_threshold:\n",
    "            # Predict age if drowsy\n",
    "            with torch.no_grad():\n",
    "                age_prediction = model(face_tensor)\n",
    "            age = int(age_prediction.item())\n",
    "            \n",
    "            # Append the predicted age to the frame's list\n",
    "            frame_ages.append(age)\n",
    "            ages.append(age)  # Append to the global ages list\n",
    "            frame_count += 1\n",
    "            \n",
    "            # Draw bounding box around the face\n",
    "            cv2.rectangle(frame, (x + 20, y), (x + w - 20, y + h + 10), (255, 0, 0), 2)\n",
    "            \n",
    "            # Draw bounding boxes around eyes if detected\n",
    "            for (ex, ey, ew, eh) in eyes:\n",
    "                cv2.rectangle(frame, (x + ex + 20, y + ey), (x + ex + ew + 20, y + ey + eh), (0, 255, 0), 2)\n",
    "\n",
    "        else:\n",
    "            # Reset ages when not drowsy\n",
    "            ages.clear()  # Clear ages if not drowsy\n",
    "            frame_count = 0  # Reset frame count when not drowsy\n",
    "\n",
    "        time.sleep(0.01)\n",
    "\n",
    "    # Check if we have enough observations for average age\n",
    "    if frame_count >= max_observations:\n",
    "        # Calculate average age\n",
    "        average_age = 1.2 * np.median(ages)\n",
    "        if average_age >= 40:\n",
    "            average_age *= 1.3\n",
    "        \n",
    "        # Display the average age on each detected face\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.putText(frame, f\"Median Age: {average_age:.1f}\", (x + 15, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Video - Face Detection and Age Prediction', frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amazonml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
